{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip trivago-clicks.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i8kEw2ZZnh_",
        "outputId": "4b076ec7-3f2b-486b-9d37-33a2efe2f340"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  trivago-clicks.zip\n",
            "   creating: trivago-clicks/\n",
            "  inflating: trivago-clicks/label-names-trivago-clicks.txt  \n",
            "  inflating: trivago-clicks/node-labels-trivago-clicks.txt  \n",
            "  inflating: trivago-clicks/README.txt  \n",
            "  inflating: trivago-clicks/hyperedges-trivago-clicks.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip cat-edge-Cooking.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSfeffdKvEmm",
        "outputId": "c796c282-7306-487f-ce56-e6b2bd492f8d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  cat-edge-Cooking.zip\n",
            "   creating: cat-edge-Cooking/\n",
            "  inflating: cat-edge-Cooking/hyperedges.txt  \n",
            "  inflating: cat-edge-Cooking/hyperedge-labels.txt  \n",
            "  inflating: cat-edge-Cooking/hyperedge-label-identities.txt  \n",
            "  inflating: cat-edge-Cooking/node-labels.txt  \n",
            "  inflating: cat-edge-Cooking/README.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnZQQ3JRGnHs",
        "outputId": "4ea987fe-2065-449a-bbcb-a814a6353e18"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L82dIRwPHEiL",
        "outputId": "795cf3db-aadc-456b-cd9c-5fe12679568b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rapidsai-csp-utils'...\n",
            "remote: Enumerating objects: 592, done.\u001b[K\n",
            "remote: Counting objects: 100% (158/158), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 592 (delta 125), reused 82 (delta 82), pack-reused 434 (from 3)\u001b[K\n",
            "Receiving objects: 100% (592/592), 194.79 KiB | 6.96 MiB/s, done.\n",
            "Resolving deltas: 100% (299/299), done.\n",
            "Installing RAPIDS remaining 25.04 libraries\n",
            "Using Python 3.11.12 environment at: /usr\n",
            "Resolved 173 packages in 3.57s\n",
            "Downloading cudf-cu12 (1.7MiB)\n",
            "Downloading cugraph-cu12 (3.0MiB)\n",
            "Downloading rmm-cu12 (1.5MiB)\n",
            "Downloading ucx-py-cu12 (2.2MiB)\n",
            "Downloading libcuspatial-cu12 (31.1MiB)\n",
            "Downloading dask (1.3MiB)\n",
            "Downloading datashader (17.5MiB)\n",
            "Downloading bokeh (6.6MiB)\n",
            "Downloading shapely (2.4MiB)\n",
            "Downloading libcuvs-cu12 (1.1GiB)\n",
            "Downloading pylibcudf-cu12 (26.4MiB)\n",
            "Downloading librmm-cu12 (2.9MiB)\n",
            "Downloading libcudf-cu12 (538.8MiB)\n",
            "Downloading libcugraph-cu12 (1.4GiB)\n",
            "Downloading cuspatial-cu12 (4.1MiB)\n",
            "Downloading raft-dask-cu12 (274.9MiB)\n",
            "Downloading cuml-cu12 (9.4MiB)\n",
            "Downloading pylibcugraph-cu12 (2.0MiB)\n",
            "Downloading cuproj-cu12 (1.1MiB)\n",
            "Downloading libkvikio-cu12 (2.0MiB)\n",
            "Downloading libcuml-cu12 (404.9MiB)\n",
            "Downloading libraft-cu12 (20.8MiB)\n",
            "Downloading cucim-cu12 (5.6MiB)\n",
            " Downloaded cuproj-cu12\n",
            " Downloaded rmm-cu12\n",
            " Downloaded shapely\n",
            " Downloaded cudf-cu12\n",
            " Downloaded pylibcugraph-cu12\n",
            " Downloaded libkvikio-cu12\n",
            " Downloaded datashader\n",
            " Downloaded dask\n",
            " Downloaded ucx-py-cu12\n",
            " Downloaded cugraph-cu12\n",
            " Downloaded bokeh\n",
            " Downloaded librmm-cu12\n",
            " Downloaded cuml-cu12\n",
            " Downloaded cuspatial-cu12\n",
            " Downloaded cucim-cu12\n",
            " Downloaded libcuspatial-cu12\n",
            " Downloaded pylibcudf-cu12\n",
            " Downloaded libraft-cu12\n",
            " Downloaded libcudf-cu12\n",
            " Downloaded raft-dask-cu12\n",
            " Downloaded libcuml-cu12\n",
            " Downloaded libcuvs-cu12\n",
            " Downloaded libcugraph-cu12\n",
            "Prepared 52 packages in 56.23s\n",
            "Uninstalled 29 packages in 764ms\n",
            "Installed 52 packages in 403ms\n",
            " + arrow==1.3.0\n",
            " - bokeh==3.7.2\n",
            " + bokeh==3.6.3\n",
            " + cucim-cu12==25.4.0\n",
            " - cudf-cu12==25.2.1 (from https://pypi.nvidia.com/cudf-cu12/cudf_cu12-25.2.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl)\n",
            " + cudf-cu12==25.4.0\n",
            " + cugraph-cu12==25.4.1\n",
            " - cuml-cu12==25.2.1\n",
            " + cuml-cu12==25.4.0\n",
            " + cuproj-cu12==25.4.0\n",
            " + cuspatial-cu12==25.4.0\n",
            " - cuvs-cu12==25.2.1\n",
            " + cuvs-cu12==25.4.0\n",
            " + cuxfilter-cu12==25.4.0\n",
            " - dask==2024.12.1\n",
            " + dask==2025.2.0\n",
            " - dask-cuda==25.2.0\n",
            " + dask-cuda==25.4.0\n",
            " - dask-cudf-cu12==25.2.2\n",
            " + dask-cudf-cu12==25.4.0\n",
            " + datashader==0.18.0\n",
            " - distributed==2024.12.1\n",
            " + distributed==2025.2.0\n",
            " - distributed-ucxx-cu12==0.42.0\n",
            " + distributed-ucxx-cu12==0.43.0\n",
            " + fqdn==1.5.1\n",
            " + isoduration==20.11.0\n",
            " - jupyter-client==6.1.12\n",
            " + jupyter-client==8.6.3\n",
            " + jupyter-events==0.12.0\n",
            " - jupyter-server==1.16.0\n",
            " + jupyter-server==2.15.0\n",
            " + jupyter-server-proxy==4.4.0\n",
            " + jupyter-server-terminals==0.5.3\n",
            " - libcudf-cu12==25.2.1 (from https://pypi.nvidia.com/libcudf-cu12/libcudf_cu12-25.2.1-py3-none-manylinux_2_28_x86_64.whl)\n",
            " + libcudf-cu12==25.4.0\n",
            " - libcugraph-cu12==25.2.0\n",
            " + libcugraph-cu12==25.4.1\n",
            " - libcuml-cu12==25.2.1\n",
            " + libcuml-cu12==25.4.0\n",
            " + libcuspatial-cu12==25.4.0\n",
            " - libcuvs-cu12==25.2.1\n",
            " + libcuvs-cu12==25.4.0\n",
            " - libkvikio-cu12==25.2.1\n",
            " + libkvikio-cu12==25.4.0\n",
            " - libraft-cu12==25.2.0\n",
            " + libraft-cu12==25.4.0\n",
            " + librmm-cu12==25.4.0\n",
            " - libucxx-cu12==0.42.0\n",
            " + libucxx-cu12==0.43.0\n",
            " - numba-cuda==0.2.0\n",
            " + numba-cuda==0.4.0\n",
            " - nx-cugraph-cu12==25.2.0 (from https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-25.2.0-py3-none-any.whl)\n",
            " + nx-cugraph-cu12==25.4.0\n",
            " + overrides==7.7.0\n",
            " + pyct==0.5.0\n",
            " - pylibcudf-cu12==25.2.1 (from https://pypi.nvidia.com/pylibcudf-cu12/pylibcudf_cu12-25.2.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl)\n",
            " + pylibcudf-cu12==25.4.0\n",
            " - pylibcugraph-cu12==25.2.0\n",
            " + pylibcugraph-cu12==25.4.1\n",
            " - pylibraft-cu12==25.2.0\n",
            " + pylibraft-cu12==25.4.0\n",
            " + python-json-logger==3.3.0\n",
            " - raft-dask-cu12==25.2.0\n",
            " + raft-dask-cu12==25.4.0\n",
            " - rapids-dask-dependency==25.2.0\n",
            " + rapids-dask-dependency==25.4.0\n",
            " + rapids-logger==0.1.1\n",
            " + rfc3339-validator==0.1.4\n",
            " + rfc3986-validator==0.1.1\n",
            " - rmm-cu12==25.2.0\n",
            " + rmm-cu12==25.4.0\n",
            " - shapely==2.1.0\n",
            " + shapely==2.0.7\n",
            " + simpervisor==1.0.0\n",
            " + types-python-dateutil==2.9.0.20241206\n",
            " - ucx-py-cu12==0.42.0\n",
            " + ucx-py-cu12==0.43.0\n",
            " - ucxx-cu12==0.42.0\n",
            " + ucxx-cu12==0.43.0\n",
            " + uri-template==1.3.0\n",
            "\n",
            "        ***********************************************************************\n",
            "        The pip install of RAPIDS is complete.\n",
            "\n",
            "        Please do not run any further installation from the conda based installation methods, as they may cause issues!\n",
            "\n",
            "        Please ensure that you're pulling from the git repo to remain updated with the latest working install scripts.\n",
            "\n",
            "        Troubleshooting:\n",
            "            - If there is an installation failure, please check back on RAPIDSAI owned templates/notebooks to see how to update your personal files.\n",
            "            - If an installation failure persists when using the latest script, please make an issue on https://github.com/rapidsai-community/rapidsai-csp-utils\n",
            "        ***********************************************************************\n",
            "        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KApDLQvXXN0M",
        "outputId": "4fcfe374-8dad-4a9e-82e5-2fec2259458d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading hyperedges…\n",
            "Loaded 233202 hyperedges\n",
            "Running GPU-accelerated MST with glue edges (float64)…\n",
            "Computing minimum spanning tree on GPU…\n",
            "Runtime        : 0.4832 s\n",
            "Total weight   : 144510.5540\n",
            "Edges in MST   : 233201 (should be 233201)\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import cudf\n",
        "import cugraph\n",
        "\n",
        "def load_hyperedges(file_path):\n",
        "    \"\"\"Load hyperedges from file and convert to list of sets.\"\"\"\n",
        "    hypers = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            hypers.append(set(map(int, line.strip().split(','))))\n",
        "    return hypers\n",
        "\n",
        "def build_bipartite_edgelist(hyperedges):\n",
        "    \"\"\"\n",
        "    cudf.DataFrame with columns ['src','dst'] for each (hyperedge,node) incidence.\n",
        "    We number hyperedges 0..H-1 and real nodes H..H+N-1.\n",
        "    \"\"\"\n",
        "    H = len(hyperedges)\n",
        "    srcs, dsts = [], []\n",
        "    for he_id, he in enumerate(hyperedges):\n",
        "        for node in he:\n",
        "            srcs.append(he_id)\n",
        "            dsts.append(node + H)\n",
        "    return cudf.DataFrame({'src': srcs, 'dst': dsts})\n",
        "\n",
        "def run_cugraph_mst(hyperedges):\n",
        "    H = len(hyperedges)\n",
        "\n",
        "    # 1) Build bipartite graph\n",
        "    edgelist = build_bipartite_edgelist(hyperedges)\n",
        "    G = cugraph.Graph(directed=False)\n",
        "    G.from_cudf_edgelist(edgelist, source='src', destination='dst', renumber=False)\n",
        "\n",
        "    # 2) Compute Jaccard similarity on the bipartite graph\n",
        "    jdf = cugraph.jaccard(G)\n",
        "\n",
        "    # 3) Keep only hyperedge–hyperedge pairs (both IDs < H)\n",
        "    mask = (jdf['first'] < H) & (jdf['second'] < H)\n",
        "    jdf_he = jdf[mask].rename(columns={'first':'src','second':'dst'})\n",
        "    # **use float64 for weight**\n",
        "    jdf_he['weight'] = (1.0 - jdf_he['jaccard_coeff']).astype('float64')\n",
        "\n",
        "    # 4) Build the sparse overlap graph (just src/dst)\n",
        "    G_j = cugraph.Graph(directed=False)\n",
        "    G_j.from_cudf_edgelist(jdf_he[['src','dst']],\n",
        "                           source='src', destination='dst',\n",
        "                           renumber=False)\n",
        "\n",
        "    # 5) Find connected components\n",
        "    comp_df = cugraph.connected_components(G_j)\n",
        "    # rename whatever the second column is to 'component'\n",
        "    comp_label_col = comp_df.columns[1]\n",
        "    comp_df = comp_df.rename(columns={comp_label_col: 'component'})\n",
        "\n",
        "    # 6) Identify the largest component by size\n",
        "    counts_df = (comp_df\n",
        "                 .groupby('component')\n",
        "                 .agg(count=('vertex','count'))\n",
        "                 .reset_index())\n",
        "    largest_comp = int(\n",
        "        counts_df.sort_values('count', ascending=False)\n",
        "                 ['component']\n",
        "                 .iloc[0]\n",
        "    )\n",
        "\n",
        "    # 7) Pick the highest-degree node in that largest component as the hub\n",
        "    deg_df = G_j.degree().rename(columns={'vertex':'vertex','degree':'degree'})\n",
        "    comp_deg = deg_df.merge(comp_df, on='vertex')\n",
        "    hub_row = (comp_deg[comp_deg['component'] == largest_comp]\n",
        "               .sort_values('degree', ascending=False)\n",
        "               .head(1))\n",
        "    hub_id = int(hub_row['vertex'].iloc[0])\n",
        "\n",
        "    # 8) Build glue‐edges (hub → every other hyperedge outside largest comp)\n",
        "    outside = comp_df[comp_df['component'] != largest_comp]['vertex']\n",
        "    glue_df = cudf.DataFrame({\n",
        "        'src':    cudf.Series([hub_id] * len(outside), dtype=outside.dtype),\n",
        "        'dst':    outside.reset_index(drop=True),\n",
        "        # **also use float64 here**\n",
        "        'weight': cudf.Series([1.0] * len(outside), dtype='float64')\n",
        "    })\n",
        "\n",
        "    # 9) Combine real overlap edges + glue edges\n",
        "    combined = cudf.concat(\n",
        "        [jdf_he[['src','dst','weight']], glue_df],\n",
        "        ignore_index=True\n",
        "    )\n",
        "\n",
        "    # 10) Build the final undirected weighted graph\n",
        "    G_w = cugraph.Graph(directed=False)\n",
        "    G_w.from_cudf_edgelist(combined,\n",
        "                           source='src',\n",
        "                           destination='dst',\n",
        "                           edge_attr='weight',\n",
        "                           renumber=False)\n",
        "\n",
        "    # 11) Run Borůvka’s MST on the augmented graph\n",
        "    print(\"Computing minimum spanning tree on GPU…\")\n",
        "    start = time.time()\n",
        "    mst_graph = cugraph.minimum_spanning_tree(G_w, weight='weight')\n",
        "    runtime = time.time() - start\n",
        "\n",
        "    # 12) Extract edges and total weight\n",
        "    mst_df = mst_graph.view_edge_list()\n",
        "    edges = list(zip(\n",
        "        mst_df['src'].to_pandas().tolist(),\n",
        "        mst_df['dst'].to_pandas().tolist(),\n",
        "        mst_df['weight'].to_pandas().tolist()\n",
        "    ))\n",
        "    total_weight = float(mst_df['weight'].sum())\n",
        "\n",
        "    return {\n",
        "        'edges': edges,\n",
        "        'total_weight': total_weight,\n",
        "        'runtime': runtime\n",
        "    }\n",
        "\n",
        "def save_tree(edges, filename):\n",
        "    \"\"\"Save MST edges to file.\"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        for u, v, w in edges:\n",
        "            f.write(f\"{u},{v},{w}\\n\")\n",
        "\n",
        "def main():\n",
        "    print(\"Loading hyperedges…\")\n",
        "    hypers = load_hyperedges('/content/trivago-clicks/hyperedges-trivago-clicks.txt')\n",
        "    print(f\"Loaded {len(hypers)} hyperedges\")\n",
        "\n",
        "    print(\"Running GPU-accelerated MST with glue edges (float64)…\")\n",
        "    result = run_cugraph_mst(hypers)\n",
        "    print(f\"Runtime        : {result['runtime']:.4f} s\")\n",
        "    print(f\"Total weight   : {result['total_weight']:.4f}\")\n",
        "    print(f\"Edges in MST   : {len(result['edges'])} (should be {len(hypers)-1})\")\n",
        "\n",
        "    save_tree(result['edges'], 'trivago_cugraph_mst_jaccard_glue.txt')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "118531.0093"
      ],
      "metadata": {
        "id": "3oDniFPISJWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import math\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "class DSU:\n",
        "    \"\"\"Disjoint-set union for finding the largest connected component.\"\"\"\n",
        "    def __init__(self, n):\n",
        "        self.parent = list(range(n))\n",
        "        self.rank = [0]*n\n",
        "\n",
        "    def find(self, x):\n",
        "        while self.parent[x] != x:\n",
        "            self.parent[x] = self.parent[self.parent[x]]\n",
        "            x = self.parent[x]\n",
        "        return x\n",
        "\n",
        "    def union(self, a, b):\n",
        "        ra, rb = self.find(a), self.find(b)\n",
        "        if ra == rb:\n",
        "            return\n",
        "        if self.rank[ra] < self.rank[rb]:\n",
        "            self.parent[ra] = rb\n",
        "        elif self.rank[ra] > self.rank[rb]:\n",
        "            self.parent[rb] = ra\n",
        "        else:\n",
        "            self.parent[rb] = ra\n",
        "            self.rank[ra] += 1\n",
        "\n",
        "def load_node_labels(path):\n",
        "    with open(path) as f:\n",
        "        return [int(line.strip()) for line in f if line.strip()]\n",
        "\n",
        "def load_label_names(path):\n",
        "    with open(path) as f:\n",
        "        return [line.rstrip(\"\\n\") for line in f]\n",
        "\n",
        "def load_hyperedges(path):\n",
        "    raw = []\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                raw.append([int(x) for x in line.split(\",\")])\n",
        "    return raw\n",
        "\n",
        "def normalize_and_keep_lcc(raw_edges, n_nodes):\n",
        "    \"\"\"\n",
        "    1) Drop any hyperedge that references an invalid node index.\n",
        "    2) Deduplicate each edge’s node list & drop singletons.\n",
        "    3) Keep only the largest connected component.\n",
        "    \"\"\"\n",
        "    # 1) filter out edges with out-of-range nodes\n",
        "    filtered = []\n",
        "    for e in raw_edges:\n",
        "        if all(0 <= v < n_nodes for v in e):\n",
        "            filtered.append(e)\n",
        "    # 2) dedup and drop size<2\n",
        "    clean = []\n",
        "    for e in filtered:\n",
        "        u = sorted(set(e))\n",
        "        if len(u) >= 2:\n",
        "            clean.append(u)\n",
        "    # 3) find LCC via DSU\n",
        "    dsu = DSU(n_nodes)\n",
        "    for e in clean:\n",
        "        a0 = e[0]\n",
        "        for a in e[1:]:\n",
        "            dsu.union(a0, a)\n",
        "    # count component sizes\n",
        "    comp_size = defaultdict(int)\n",
        "    for i in range(n_nodes):\n",
        "        comp_size[dsu.find(i)] += 1\n",
        "    main = max(comp_size, key=comp_size.get)\n",
        "    valid_nodes = {i for i in range(n_nodes) if dsu.find(i) == main}\n",
        "    # keep only edges fully in main CC\n",
        "    final = [e for e in clean if all(v in valid_nodes for v in e)]\n",
        "    return final, valid_nodes\n",
        "\n",
        "def compute_hyperedge_labels(edges, node_labels):\n",
        "    \"\"\"\n",
        "    Assign each hyperedge the mode of its nodes’ labels.\n",
        "    Tie → smallest label.\n",
        "    \"\"\"\n",
        "    hed_labels = []\n",
        "    for e in edges:\n",
        "        cnt = Counter(node_labels[v] for v in e)\n",
        "        mode = min((lbl for lbl, _ in cnt.items() if cnt[lbl] == max(cnt.values())))\n",
        "        hed_labels.append(mode)\n",
        "    return hed_labels\n",
        "\n",
        "def stratified_sample_indices(stratum_labels, target_count, seed=42):\n",
        "    \"\"\"\n",
        "    Stratify indices by stratum_labels[i], guarantee ≥1 per label,\n",
        "    bump target_count if needed, then fill randomly.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    m = len(stratum_labels)\n",
        "    lbl2idxs = defaultdict(list)\n",
        "    for i, lbl in enumerate(stratum_labels):\n",
        "        lbl2idxs[lbl].append(i)\n",
        "    usable = list(lbl2idxs)\n",
        "    if target_count < len(usable):\n",
        "        print(f\"⚠️  target_count={target_count} < #labels={len(usable)}, bumping to {len(usable)}\")\n",
        "        target_count = len(usable)\n",
        "    selected = set()\n",
        "    for lbl in usable:\n",
        "        selected.add(random.choice(lbl2idxs[lbl]))\n",
        "        if len(selected) == target_count:\n",
        "            break\n",
        "    need = target_count - len(selected)\n",
        "    if need > 0:\n",
        "        remaining = list(set(range(m)) - selected)\n",
        "        selected.update(random.sample(remaining, need))\n",
        "    return selected\n",
        "\n",
        "def write_hyperedges(eids, edges, path):\n",
        "    with open(path, 'w') as f:\n",
        "        for i in sorted(eids):\n",
        "            f.write(\",\".join(map(str, edges[i])) + \"\\n\")\n",
        "\n",
        "def write_node_labels_from_edges(eids, edges, node_labels, path):\n",
        "    nodes = set()\n",
        "    for i in eids:\n",
        "        nodes.update(edges[i])\n",
        "    with open(path, 'w') as f:\n",
        "        for v in sorted(nodes):\n",
        "            f.write(f\"{node_labels[v]}\\n\")\n",
        "\n",
        "def write_label_names(label_names, path):\n",
        "    with open(path, 'w') as f:\n",
        "        for nm in label_names:\n",
        "            f.write(nm + \"\\n\")\n",
        "\n",
        "def main():\n",
        "    ### === CONFIGURE HERE ===\n",
        "    node_labels_path   = '/content/trivago-clicks/node-labels-trivago-clicks.txt'\n",
        "    label_names_path   = '/content/trivago-clicks/label-names-trivago-clicks.txt'\n",
        "    hyperedges_path    = '/content/trivago-clicks/hyperedges-trivago-clicks.txt'\n",
        "    output_prefix      = \"sample10pct\"\n",
        "    sample_fraction    = 0.50\n",
        "    random_seed        = 123\n",
        "    ### ====================\n",
        "\n",
        "    # Load\n",
        "    node_labels  = load_node_labels(node_labels_path)\n",
        "    label_names  = load_label_names(label_names_path)\n",
        "    raw_edges    = load_hyperedges(hyperedges_path)\n",
        "\n",
        "    # Normalize + LCC\n",
        "    edges, valid_nodes = normalize_and_keep_lcc(raw_edges, len(node_labels))\n",
        "    print(f\"→ {len(raw_edges)} raw → {len(edges)} valid hyperedges; {len(valid_nodes)} nodes in main CC\")\n",
        "\n",
        "    # Label each hyperedge\n",
        "    hed_labels = compute_hyperedge_labels(edges, node_labels)\n",
        "\n",
        "    # Sample 10% stratified by hyperedge-label\n",
        "    total = len(edges)\n",
        "    target = max(1, int(math.floor(total * sample_fraction)))\n",
        "    sel = stratified_sample_indices(hed_labels, target, seed=random_seed)\n",
        "    print(f\"→ selected {len(sel)}/{total} hyperedges ({sample_fraction*100:.1f}%)\")\n",
        "\n",
        "    # Write outputs\n",
        "    write_hyperedges(sel, edges,                     f\"hyperedges-{output_prefix}.txt\")\n",
        "    write_node_labels_from_edges(sel, edges, node_labels, f\"node-labels-{output_prefix}.txt\")\n",
        "    write_label_names(label_names,                    f\"label-names-{output_prefix}.txt\")\n",
        "\n",
        "    print(\"✅ Done.\")\n",
        "    print(f\"  hyperedges-{output_prefix}.txt\")\n",
        "    print(f\"  node-labels-{output_prefix}.txt\")\n",
        "    print(f\"  label-names-{output_prefix}.txt\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDy7X9pElNXT",
        "outputId": "c8288749-5df0-40f1-cbe0-ae8dbea73ab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ 233202 raw → 233198 valid hyperedges; 172733 nodes in main CC\n",
            "→ selected 23319/233198 hyperedges (10.0%)\n",
            "✅ Done.\n",
            "  hyperedges-sample10pct.txt\n",
            "  node-labels-sample10pct.txt\n",
            "  label-names-sample10pct.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6yW9Pu6oM52O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Set\n",
        "\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix as sp_csr\n",
        "from cupyx.scipy.sparse import csr_matrix as cp_csr\n",
        "from cuml.metrics import sparse_pairwise_distances\n",
        "import cugraph\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "###############################\n",
        "#          SETTINGS           #\n",
        "###############################\n",
        "DATA_DIR = Path(\"/content/trivago-clicks\")\n",
        "DATASET_PREFIX = \"trivago-clicks\"  # file name stem\n",
        "\n",
        "OUT_DIR = Path(\"outputs\")\n",
        "PARTITION_DIR = OUT_DIR / \"partitions\"\n",
        "PARTITION_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "###############################\n",
        "#     DATA LOADING HELPERS    #\n",
        "###############################\n",
        "\n",
        "def load_hyperedges(path: Path) -> List[Set[int]]:\n",
        "    \"\"\"Return list of hyperedges as sets of ints (node IDs).\"\"\"\n",
        "    hypers: List[Set[int]] = []\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                hypers.append(set(map(int, line.split(\",\"))))\n",
        "    return hypers\n",
        "\n",
        "\n",
        "def load_node_labels(path: Path) -> List[str]:\n",
        "    \"\"\"Return list where index=node ID and value=label string.\"\"\"\n",
        "    with open(path) as f:\n",
        "        return [ln.strip() for ln in f]\n",
        "\n",
        "\n",
        "def majority_label(hyperedge: Set[int], node_labels: List[str]) -> str:\n",
        "    \"\"\"Label hyperedge by the most frequent node label (lexicographic tie‑break).\"\"\"\n",
        "    cnt = Counter(node_labels[n] if n < len(node_labels) else \"UNKNOWN\" for n in hyperedge)\n",
        "    if not cnt:\n",
        "        return \"UNKNOWN\"\n",
        "    max_c = max(cnt.values())\n",
        "    return min(lbl for lbl, c in cnt.items() if c == max_c)\n",
        "\n",
        "###############################\n",
        "#   GRAPH + PARTITION BUILD   #\n",
        "###############################\n",
        "\n",
        "def build_incidence_and_labels() -> Tuple[sp_csr, np.ndarray, List[Set[int]]]:\n",
        "    \"\"\"Load dataset, fix 1‑based IDs, build CSR incidence & int labels array.\"\"\"\n",
        "    hyper_path = DATA_DIR / f\"hyperedges-{DATASET_PREFIX}.txt\"\n",
        "    label_path = DATA_DIR / f\"node-labels-{DATASET_PREFIX}.txt\"\n",
        "\n",
        "    hypers = load_hyperedges(hyper_path)\n",
        "    node_labels = load_node_labels(label_path)\n",
        "\n",
        "    # detect 1‑based: if ANY node id >= len(node_labels)\n",
        "    n_nodes_file = len(node_labels)\n",
        "    if any(n >= n_nodes_file for he in hypers for n in he):\n",
        "        hypers = [set(n - 1 for n in he) for he in hypers]\n",
        "\n",
        "    # incidence rows/cols\n",
        "    rows, cols = [], []\n",
        "    for hid, he in enumerate(hypers):\n",
        "        rows.extend([hid] * len(he))\n",
        "        cols.extend(he)\n",
        "    data = np.ones(len(rows), dtype=np.int8)\n",
        "    n_hyperedges = len(hypers)\n",
        "    n_nodes = max(cols) + 1\n",
        "    H = sp_csr((data, (rows, cols)), shape=(n_hyperedges, n_nodes))\n",
        "\n",
        "    # string labels → ints\n",
        "    str_labs = [majority_label(he, node_labels) for he in hypers]\n",
        "    uniq = {lbl: i for i, lbl in enumerate(sorted(set(str_labs)))}\n",
        "    int_labels = np.array([uniq[lbl] for lbl in str_labs], dtype=np.int32)\n",
        "    return H, int_labels, hypers\n",
        "\n",
        "###############################\n",
        "#   INTERNAL MST (GPU Prim)   #\n",
        "###############################\n",
        "\n",
        "def prim_exact_mst(X_csr: cp_csr, metric: str = \"jaccard\") -> List[Tuple[int, int, float]]:\n",
        "    n = X_csr.shape[0]\n",
        "    if n == 1:\n",
        "        return []\n",
        "    in_tree = cp.zeros(n, dtype=cp.bool_)\n",
        "    in_tree[0] = True\n",
        "\n",
        "    dist = sparse_pairwise_distances(X_csr[0:1], X_csr, metric=metric).ravel()\n",
        "    parent = cp.zeros(n, dtype=cp.int32)\n",
        "    edges = []\n",
        "    for _ in range(1, n):\n",
        "        u = int(cp.argmin(cp.where(in_tree, cp.inf, dist)).get())\n",
        "        edges.append((int(parent[u].get()), u, float(dist[u].get())))\n",
        "        in_tree[u] = True\n",
        "        du = sparse_pairwise_distances(X_csr[u:u+1], X_csr, metric=metric).ravel()\n",
        "        mask = (~in_tree) & (du < dist)\n",
        "        dist = cp.where(mask, du, dist)\n",
        "        parent = cp.where(mask, u, parent)\n",
        "    return edges\n",
        "\n",
        "###############################\n",
        "#   COARSENED GRAPH HELPERS   #\n",
        "###############################\n",
        "\n",
        "def min_distance(hypers: List[Set[int]], jdict: Dict[Tuple[int, int], float], A: List[int], rep_B: int) -> float:\n",
        "    \"\"\"Min distance from any hyperedge in A to representative rep_B.\"\"\"\n",
        "    return min(jdict.get((u, rep_B), jdict.get((rep_B, u), 1.0)) for u in A)\n",
        "\n",
        "###############################\n",
        "#              MAIN           #\n",
        "###############################\n",
        "\n",
        "def main():\n",
        "    t0 = time.perf_counter()\n",
        "    H, labels, hypers = build_incidence_and_labels()\n",
        "    load_time = time.perf_counter() - t0\n",
        "\n",
        "    # build partitions\n",
        "    part: Dict[int, List[int]] = defaultdict(list)\n",
        "    for idx, lab in enumerate(labels):\n",
        "        part[int(lab)].append(idx)\n",
        "\n",
        "    # compute Jaccard matrix for entire set using helper from original pipeline\n",
        "    from typing import Dict as _Dict, Tuple as _Tuple\n",
        "    def build_bipartite_df(hs: List[Set[int]]):\n",
        "        src, dst = [], []\n",
        "        HN = len(hs)\n",
        "        for hid, he in enumerate(hs):\n",
        "            for n in he:\n",
        "                src.append(hid)\n",
        "                dst.append(n + HN)\n",
        "        return cudf.DataFrame({\"src\": src, \"dst\": dst})\n",
        "\n",
        "    def get_global_jaccard(hs: List[Set[int]]) -> _Dict[_Tuple[int,int], float]:\n",
        "        HN = len(hs)\n",
        "        if HN <= 1:\n",
        "            return {}\n",
        "        df = build_bipartite_df(hs)\n",
        "        G = cugraph.Graph(directed=False)\n",
        "        G.from_cudf_edgelist(df, source='src', destination='dst', renumber=False)\n",
        "        jdf = cugraph.jaccard(G)\n",
        "        mask = (jdf['first'] < HN) & (jdf['second'] < HN)\n",
        "        jdf = jdf[mask]\n",
        "        jdf['weight'] = (1.0 - jdf['jaccard_coeff']).astype('float32')\n",
        "        pdf = jdf[['first','second','weight']].to_pandas()\n",
        "        return {(int(u), int(v)): float(w) for u, v, w in pdf.itertuples(index=False)}\n",
        "\n",
        "    jdict = get_global_jaccard(hypers)\n",
        "\n",
        "    # Component MSTs\n",
        "    H_gpu = cp_csr(H.astype(np.float32))\n",
        "    comp_edges: List[Tuple[int,int,float]] = []\n",
        "    comp_weight = 0.0\n",
        "    for lab, idxs in part.items():\n",
        "        sub_gpu = H_gpu[idxs]\n",
        "        e = prim_exact_mst(sub_gpu)\n",
        "        comp_edges.extend([(idxs[s], idxs[d], w) for s,d,w in e])\n",
        "        comp_weight += sum(w for _,_,w in e)\n",
        "\n",
        "    # Coarsened MST\n",
        "    reps = {lab: idxs[0] for lab, idxs in part.items()}\n",
        "    labs = list(part.keys())\n",
        "    rows = []; cols = []; wts = []\n",
        "    for i, li in enumerate(labs[:-1]):\n",
        "        Pi = part[li]\n",
        "        for j, lj in enumerate(labs[i+1:], i+1):\n",
        "            Pj = part[lj]\n",
        "            wi = min_distance(hypers, jdict, Pi, reps[lj])\n",
        "            wj = min_distance(hypers, jdict, Pj, reps[li])\n",
        "            w = min(wi, wj, 1.0)\n",
        "            rows.append(i); cols.append(j); wts.append(w)\n",
        "    from scipy.sparse.csgraph import minimum_spanning_tree\n",
        "    M = sp_csr((wts, (rows, cols)), shape=(len(labs), len(labs)))\n",
        "    M = M + M.T\n",
        "    mst_small = minimum_spanning_tree(M).toarray()\n",
        "    coarse_edges = []\n",
        "    coarse_weight = 0.0\n",
        "    for i in range(len(labs)):\n",
        "        for j in range(len(labs)):\n",
        "            w = mst_small[i, j]\n",
        "            if w > 0:\n",
        "                coarse_edges.append((reps[labs[i]], reps[labs[j]], float(w)))\n",
        "                coarse_weight += w\n",
        "\n",
        "    # Combined\n",
        "    total_mfc = comp_weight + coarse_weight\n",
        "\n",
        "    print(f\"Load time: {load_time:.2f}s  |  Hyperedges: {len(hypers):,}  Partitions: {len(part)}\")\n",
        "    print(f\"Component MST weight: {comp_weight:.4f}  |  Coarse MST weight: {coarse_weight:.4f}  |  MFC total: {total_mfc:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuMJ0yt5Pm9F",
        "outputId": "baa667ff-b9a7-4734-ea00-61ae051e6121"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "233,202 hyperedges, 144 partitions\n",
            "1) Computing exact global MST...\n",
            "   OPT MST weight = 144510.5540\n",
            "2) Computing partition MSTs...\n",
            "   Sum cluster MST weights = 166507.0513\n",
            "3) Computing coarsened MST...\n",
            "   Coarse MST weight = 90.3475\n",
            "4) MFC total weight = 166597.39885097742\n",
            "   beta (MFC/OPT)    = 1.1528389746457437\n",
            "   gamma (intra)     = 3.1548123102593326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "from cuml.metrics import sparse_pairwise_distances\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "from scipy.sparse import csr_matrix as sp_csr\n",
        "from cupyx.scipy.sparse import csr_matrix as cp_csr\n",
        "from cuml.metrics import sparse_pairwise_distances\n",
        "from cuml.neighbors import NearestNeighbors\n",
        "import cudf\n",
        "import cugraph\n",
        "\n",
        "# 1) Load your hyperedges.txt into a SciPy CSR matrix\n",
        "#    – each line of hyperedges.txt is a whitespace-separated list of ingredient IDs\n",
        "rows = []\n",
        "cols = []\n",
        "with open('/content/cat-edge-Cooking/hyperedges.txt', 'r') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        ing = [int(x) for x in line.strip().split()]\n",
        "        rows.extend([i] * len(ing))\n",
        "        cols.extend(ing)\n",
        "data = np.ones(len(rows), dtype=np.int8)\n",
        "n_recipes = i + 1\n",
        "n_ingredients = max(cols) + 1\n",
        "H = sp_csr((data, (rows, cols)),\n",
        "           shape=(n_recipes, n_ingredients))\n",
        "def prim_exact_mst(X_csr, metric='jaccard'):\n",
        "    n = X_csr.shape[0]\n",
        "    in_tree = cp.zeros(n, dtype=cp.bool_)\n",
        "    in_tree[0] = True\n",
        "\n",
        "    # dist[v] = best distance from v to any node in the current tree\n",
        "    dist   = cp.full(n, cp.inf, dtype=cp.float32)\n",
        "    parent = cp.full(n, -1, dtype=cp.int32)\n",
        "\n",
        "    # initialize distances from node 0\n",
        "    d0 = sparse_pairwise_distances(X_csr[0:1], X_csr, metric=metric).ravel()\n",
        "    dist = d0\n",
        "    parent[:] = 0\n",
        "\n",
        "    edges = []\n",
        "    for _ in tqdm(range(1, n)):\n",
        "        # pick the closest outside-tree node\n",
        "        dists = cp.where(in_tree, cp.inf, dist)\n",
        "        u = int(cp.argmin(dists).get())\n",
        "\n",
        "        # record the MST edge (parent[u] → u)\n",
        "        edges.append((int(parent[u].get()), u, float(dist[u].get())))\n",
        "        in_tree[u] = True\n",
        "\n",
        "        # update distances using the newly added node u\n",
        "        du = sparse_pairwise_distances(X_csr[u:u+1], X_csr, metric=metric).ravel()\n",
        "        mask = (~in_tree) & (du < dist)\n",
        "        dist   = cp.where(mask, du, dist)\n",
        "        parent = cp.where(mask, u, parent)\n",
        "\n",
        "    return edges  # list of (src, dst, weight)\n",
        "\n",
        "# Usage:\n",
        "from cupyx.scipy.sparse import csr_matrix as cp_csr\n",
        "H_gpu = cp_csr(H.astype(float))\n",
        "mst_edges = prim_exact_mst(H_gpu, metric='jaccard')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HLw2lbH9HuQ",
        "outputId": "5949128c-0c85-453a-c012-5d9a2f9e83b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/cuml/internals/api_decorators.py:213: UserWarning: Y was converted to boolean for metric jaccard\n",
            "  ret = func(*args, **kwargs)\n",
            "  0%|          | 11/39773 [00:00<12:04, 54.87it/s]/usr/local/lib/python3.11/dist-packages/cuml/internals/api_decorators.py:213: UserWarning: X was converted to boolean for metric jaccard\n",
            "  ret = func(*args, **kwargs)\n",
            "100%|██████████| 39773/39773 [05:14<00:00, 126.39it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save prims tree\n",
        "w = 0\n",
        "with open('prims_tree_cooking.txt', 'w') as f:\n",
        "\n",
        "    for edge in mst_edges:\n",
        "        w += edge[2]\n",
        "        f.write(f\"{edge[0]} {edge[1]} {edge[2]}\\n\")"
      ],
      "metadata": {
        "id": "06-nlmpD-WTh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD1csTc8GLJ9",
        "outputId": "e727dda5-dee3-48d3-9231-77cfee9f41ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24054.699267568587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi3752PV-e3H",
        "outputId": "d47922ac-2265-4482-8f0d-7af29fe25d9e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2793.5656913811167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix as sp_csr\n",
        "from cupyx.scipy.sparse import csr_matrix as cp_csr\n",
        "from cuml.metrics import sparse_pairwise_distances\n",
        "import cugraph\n",
        "\n",
        "###############################\n",
        "#          SETTINGS           #\n",
        "###############################\n",
        "DATA_DIR = Path(\"/content/cat-edge-Cooking\")  # change if needed\n",
        "OUT_DIR = Path(\"outputs\")\n",
        "PARTITION_DIR = OUT_DIR / \"partitions\"\n",
        "PARTITION_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "###############################\n",
        "#     UTILITY  FUNCTIONS     #\n",
        "###############################\n",
        "\n",
        "def load_hypergraph() -> Tuple[sp_csr, np.ndarray]:\n",
        "    \"\"\"Load hyperedges CSR matrix H and hyperedge labels.\"\"\"\n",
        "    rows, cols = [], []\n",
        "    with open(DATA_DIR / \"hyperedges.txt\", \"r\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            ing = [int(x) for x in line.strip().split()]\n",
        "            rows.extend([i] * len(ing))\n",
        "            cols.extend(ing)\n",
        "    data = np.ones(len(rows), dtype=np.int8)\n",
        "    n_hyperedges = i + 1\n",
        "    n_nodes = max(cols) + 1\n",
        "    H = sp_csr((data, (rows, cols)), shape=(n_hyperedges, n_nodes))\n",
        "\n",
        "    labels = np.loadtxt(DATA_DIR / \"hyperedge-labels.txt\", dtype=np.int32)\n",
        "    assert labels.shape[0] == n_hyperedges, \"Label / hyperedge size mismatch!\"\n",
        "    return H, labels\n",
        "\n",
        "\n",
        "def prim_exact_mst(X_csr: cp_csr, metric: str = \"jaccard\") -> List[Tuple[int, int, float]]:\n",
        "    \"\"\"GPU‑accelerated Prim's algorithm; returns edges as (src, dst, weight).\"\"\"\n",
        "    n = X_csr.shape[0]\n",
        "    in_tree = cp.zeros(n, dtype=cp.bool_)\n",
        "    in_tree[0] = True\n",
        "\n",
        "    dist = sparse_pairwise_distances(X_csr[0:1], X_csr, metric=metric).ravel()\n",
        "    parent = cp.zeros(n, dtype=cp.int32)\n",
        "\n",
        "    edges: List[Tuple[int, int, float]] = []\n",
        "    for _ in range(1, n):\n",
        "        u = int(cp.argmin(cp.where(in_tree, cp.inf, dist)).get())\n",
        "        edges.append((int(parent[u].get()), u, float(dist[u].get())))\n",
        "        in_tree[u] = True\n",
        "        du = sparse_pairwise_distances(X_csr[u:u + 1], X_csr, metric=metric).ravel()\n",
        "        mask = (~in_tree) & (du < dist)\n",
        "        dist = cp.where(mask, du, dist)\n",
        "        parent = cp.where(mask, u, parent)\n",
        "    return edges\n",
        "\n",
        "\n",
        "def build_partitions(labels: np.ndarray) -> Dict[int, List[int]]:\n",
        "    partitions: Dict[int, List[int]] = {}\n",
        "    for idx, lab in enumerate(labels):\n",
        "        partitions.setdefault(lab, []).append(idx)\n",
        "    return partitions\n",
        "\n",
        "\n",
        "def component_representatives(partitions: Dict[int, List[int]]) -> Dict[int, int]:\n",
        "    return {lab: nodes[0] for lab, nodes in partitions.items()}\n",
        "\n",
        "\n",
        "def min_distance_to_rep(X_gpu: cp_csr, nodes: List[int], rep_idx: int) -> float:\n",
        "    sub = X_gpu[nodes]\n",
        "    rep = X_gpu[rep_idx:rep_idx + 1]\n",
        "    d = sparse_pairwise_distances(rep, sub, metric=\"jaccard\")\n",
        "    return float(d.min().get())\n",
        "\n",
        "\n",
        "def coarsen_graph(H_gpu: cp_csr, partitions: Dict[int, List[int]]) -> List[Tuple[int, int, float]]:\n",
        "    reps = component_representatives(partitions)\n",
        "    labels = sorted(partitions.keys())\n",
        "    edges = []\n",
        "    for i_idx, li in enumerate(labels[:-1]):\n",
        "        for lj in labels[i_idx + 1:]:\n",
        "            wi_j = min_distance_to_rep(H_gpu, partitions[li], reps[lj])\n",
        "            wj_i = min_distance_to_rep(H_gpu, partitions[lj], reps[li])\n",
        "            edges.append((li, lj, min(wi_j, wj_i)))\n",
        "    return edges\n",
        "\n",
        "\n",
        "def mst_on_coarsened(num_components: int, edges: List[Tuple[int, int, float]]) -> List[Tuple[int, int, float]]:\n",
        "    \"\"\"Run MST on coarsened graph using cuGraph and return edge list tuples.\"\"\"\n",
        "    if num_components <= 1:\n",
        "        return []\n",
        "\n",
        "    import cudf\n",
        "    gdf = cudf.DataFrame(edges, columns=[\"src\", \"dst\", \"weight\"])\n",
        "    G = cugraph.Graph()\n",
        "    G.from_cudf_edgelist(gdf, source=\"src\", destination=\"dst\", edge_attr=\"weight\", renumber=False)\n",
        "\n",
        "    # cuGraph returns a Graph; extract its edge list\n",
        "    mst_graph = cugraph.minimum_spanning_tree(G)\n",
        "    elist = mst_graph.view_edge_list()\n",
        "\n",
        "    # ensure consistent column names\n",
        "    elist = elist.rename(columns={\"weights\": \"weight\"})\n",
        "\n",
        "    return list(elist.to_pandas()[[\"src\", \"dst\", \"weight\"]].itertuples(index=False, name=None))\n",
        "\n",
        "###############################\n",
        "#           MAIN              #\n",
        "###############################\n",
        "\n",
        "def main():\n",
        "    timings = {}\n",
        "\n",
        "    # STEP 0: load data\n",
        "    t0 = time.perf_counter()\n",
        "    H, labels = load_hypergraph()\n",
        "    timings[\"load_data\"] = time.perf_counter() - t0\n",
        "\n",
        "    H_gpu = cp_csr(H.astype(np.float32))\n",
        "\n",
        "    # STEP 1: partition hyperedges\n",
        "    t0 = time.perf_counter()\n",
        "    partitions = build_partitions(labels)\n",
        "    timings[\"partition\"] = time.perf_counter() - t0\n",
        "\n",
        "    mapping_path = OUT_DIR / \"hyperedge_partition_mapping.txt\"\n",
        "    with open(mapping_path, \"w\") as f:\n",
        "        for h_idx, lab in enumerate(labels):\n",
        "            f.write(f\"{h_idx}\\t{lab}\\n\")\n",
        "\n",
        "    # STEP 2: internal MSTs\n",
        "    internal_edges: Dict[int, List[Tuple[int, int, float]]] = {}\n",
        "    internal_weights: Dict[int, float] = {}\n",
        "    mst_time = 0.0\n",
        "    for lab, nodes in partitions.items():\n",
        "        if len(nodes) == 1:\n",
        "            internal_edges[lab] = []\n",
        "            internal_weights[lab] = 0.0\n",
        "            continue\n",
        "        t0 = time.perf_counter()\n",
        "        sub_gpu = H_gpu[nodes]\n",
        "        edges = prim_exact_mst(sub_gpu)\n",
        "        mst_time += time.perf_counter() - t0\n",
        "        internal_edges[lab] = [(nodes[s], nodes[d], w) for s, d, w in edges]\n",
        "        internal_weights[lab] = sum(w for _, _, w in edges)\n",
        "        (PARTITION_DIR / f\"partition_{lab}_mst.txt\").write_text(\n",
        "            \"\\n\".join(f\"{s} {d} {w}\" for s, d, w in internal_edges[lab]))\n",
        "\n",
        "    timings[\"component_msts\"] = mst_time\n",
        "\n",
        "    # STEP 3: coarsen + MST\n",
        "    t0 = time.perf_counter()\n",
        "    coarsened_edges = coarsen_graph(H_gpu, partitions)\n",
        "    timings[\"compute_coarsened_edges\"] = time.perf_counter() - t0\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    comp_mst_edges = mst_on_coarsened(len(partitions), coarsened_edges)\n",
        "    timings[\"mst_coarsened\"] = time.perf_counter() - t0\n",
        "\n",
        "    # STEP 4: combine\n",
        "    combined_edges = [edge for edges in internal_edges.values() for edge in edges]\n",
        "    combined_edges.extend(comp_mst_edges)\n",
        "\n",
        "    approx_mfc_weight = sum(w for *_, w in combined_edges)\n",
        "    coarsened_weight = sum(internal_weights.values())\n",
        "\n",
        "    (OUT_DIR / \"combined_mst.txt\").write_text(\n",
        "        \"\\n\".join(f\"{s} {d} {w}\" for s, d, w in combined_edges))\n",
        "\n",
        "    stats_lines = [\n",
        "        *[f\"{k}: {v:.4f} s\" for k, v in timings.items()],\n",
        "        f\"approx_MFC_weight: {approx_mfc_weight:.6f}\",\n",
        "        f\"coarsened_graph_weight: {coarsened_weight:.6f}\",\n",
        "    ]\n",
        "    (OUT_DIR / \"run_stats.txt\").write_text(\"\\n\".join(stats_lines))\n",
        "\n",
        "    print(\"=== Timings (s) ===\")\n",
        "    for k, v in timings.items():\n",
        "        print(f\"{k:<25} {v:.4f}\")\n",
        "    print(\"\\napprox MFC weight:\", approx_mfc_weight)\n",
        "    print(\"coarsened graph weight:\", coarsened_weight)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYOuVN6p4UlP",
        "outputId": "55e4393f-d4b4-4a94-b8a3-308320797e93"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/cuml/internals/api_decorators.py:213: UserWarning: Y was converted to boolean for metric jaccard\n",
            "  ret = func(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/cuml/internals/api_decorators.py:213: UserWarning: X was converted to boolean for metric jaccard\n",
            "  ret = func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Timings (s) ===\n",
            "load_data                 0.3632\n",
            "partition                 0.0129\n",
            "component_msts            149.9735\n",
            "compute_coarsened_edges   1.3936\n",
            "mst_coarsened             0.0984\n",
            "\n",
            "approx MFC weight: 25084.849987387657\n",
            "coarsened graph weight: 25074.57403945923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/outputs_cooking.zip /content/outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXvC8mTfAIXM",
        "outputId": "44164d86-b98a-414a-f676-40c55d6cb00f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/outputs/ (stored 0%)\n",
            "  adding: content/outputs/partitions/ (stored 0%)\n",
            "  adding: content/outputs/partitions/partition_19_mst.txt (deflated 72%)\n",
            "  adding: content/outputs/partitions/partition_8_mst.txt (deflated 73%)\n",
            "  adding: content/outputs/partitions/partition_4_mst.txt (deflated 73%)\n",
            "  adding: content/outputs/partitions/partition_10_mst.txt (deflated 73%)\n",
            "  adding: content/outputs/partitions/partition_16_mst.txt (deflated 74%)\n",
            "  adding: content/outputs/partitions/partition_17_mst.txt (deflated 74%)\n",
            "  adding: content/outputs/partitions/partition_5_mst.txt (deflated 72%)\n",
            "  adding: content/outputs/partitions/partition_1_mst.txt (deflated 73%)\n",
            "  adding: content/outputs/partitions/partition_2_mst.txt (deflated 72%)\n",
            "  adding: content/outputs/partitions/partition_20_mst.txt (deflated 75%)\n",
            "  adding: content/outputs/partitions/partition_18_mst.txt (deflated 74%)\n",
            "  adding: content/outputs/partitions/partition_15_mst.txt (deflated 74%)\n",
            "  adding: content/outputs/partitions/partition_13_mst.txt (deflated 75%)\n",
            "  adding: content/outputs/partitions/partition_6_mst.txt (deflated 74%)\n",
            "  adding: content/outputs/partitions/partition_12_mst.txt (deflated 74%)\n",
            "  adding: content/outputs/partitions/partition_9_mst.txt (deflated 74%)\n",
            "  adding: content/outputs/partitions/partition_3_mst.txt (deflated 72%)\n",
            "  adding: content/outputs/partitions/partition_11_mst.txt (deflated 74%)\n",
            "  adding: content/outputs/partitions/partition_14_mst.txt (deflated 74%)\n",
            "  adding: content/outputs/partitions/partition_7_mst.txt (deflated 75%)\n",
            "  adding: content/outputs/run_stats.txt (deflated 28%)\n",
            "  adding: content/outputs/combined_mst.txt (deflated 75%)\n",
            "  adding: content/outputs/hyperedge_partition_mapping.txt (deflated 65%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Set\n",
        "\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix as sp_csr\n",
        "from cupyx.scipy.sparse import csr_matrix as cp_csr\n",
        "from cuml.metrics import sparse_pairwise_distances\n",
        "import cugraph\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "###############################\n",
        "#          SETTINGS           #\n",
        "###############################\n",
        "DATA_DIR = Path(\"/content/trivago-clicks\")\n",
        "DATASET_PREFIX = \"trivago-clicks\"  # file name stem\n",
        "\n",
        "OUT_DIR = Path(\"outputs_trivago\")\n",
        "PARTITION_DIR = OUT_DIR / \"partitions\"\n",
        "PARTITION_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "###############################\n",
        "#     DATA LOADING HELPERS    #\n",
        "###############################\n",
        "\n",
        "def load_hyperedges(path: Path) -> List[Set[int]]:\n",
        "    \"\"\"Return list of hyperedges as sets of ints (node IDs).\"\"\"\n",
        "    hypers: List[Set[int]] = []\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                hypers.append(set(map(int, line.split(\",\"))))\n",
        "    return hypers\n",
        "\n",
        "\n",
        "def load_node_labels(path: Path) -> List[str]:\n",
        "    \"\"\"Return list where index=node ID and value=label string.\"\"\"\n",
        "    with open(path) as f:\n",
        "        return [ln.strip() for ln in f]\n",
        "\n",
        "\n",
        "def majority_label(hyperedge: Set[int], node_labels: List[str]) -> str:\n",
        "    \"\"\"Label hyperedge by the most frequent node label (lexicographic tie‑break).\"\"\"\n",
        "    cnt = Counter(node_labels[n] if n < len(node_labels) else \"UNKNOWN\" for n in hyperedge)\n",
        "    if not cnt:\n",
        "        return \"UNKNOWN\"\n",
        "    max_c = max(cnt.values())\n",
        "    return min(lbl for lbl, c in cnt.items() if c == max_c)\n",
        "\n",
        "###############################\n",
        "#   GRAPH + PARTITION BUILD   #\n",
        "###############################\n",
        "\n",
        "def build_incidence_and_labels() -> Tuple[sp_csr, np.ndarray, List[Set[int]]]:\n",
        "    \"\"\"Load dataset, fix 1‑based IDs, build CSR incidence & int labels array.\"\"\"\n",
        "    hyper_path = DATA_DIR / f\"hyperedges-{DATASET_PREFIX}.txt\"\n",
        "    label_path = DATA_DIR / f\"node-labels-{DATASET_PREFIX}.txt\"\n",
        "\n",
        "    hypers = load_hyperedges(hyper_path)\n",
        "    node_labels = load_node_labels(label_path)\n",
        "\n",
        "    # detect 1‑based: if ANY node id >= len(node_labels)\n",
        "    n_nodes_file = len(node_labels)\n",
        "    if any(n >= n_nodes_file for he in hypers for n in he):\n",
        "        hypers = [set(n - 1 for n in he) for he in hypers]\n",
        "\n",
        "    # incidence rows/cols\n",
        "    rows, cols = [], []\n",
        "    for hid, he in enumerate(hypers):\n",
        "        rows.extend([hid] * len(he))\n",
        "        cols.extend(he)\n",
        "    data = np.ones(len(rows), dtype=np.int8)\n",
        "    n_hyperedges = len(hypers)\n",
        "    n_nodes = max(cols) + 1\n",
        "    H = sp_csr((data, (rows, cols)), shape=(n_hyperedges, n_nodes))\n",
        "\n",
        "    # string labels → ints\n",
        "    str_labs = [majority_label(he, node_labels) for he in hypers]\n",
        "    uniq = {lbl: i for i, lbl in enumerate(sorted(set(str_labs)))}\n",
        "    int_labels = np.array([uniq[lbl] for lbl in str_labs], dtype=np.int32)\n",
        "    return H, int_labels, hypers\n",
        "\n",
        "###############################\n",
        "#   INTERNAL MST (GPU Prim)   #\n",
        "###############################\n",
        "\n",
        "def prim_exact_mst(X_csr: cp_csr, metric: str = \"jaccard\") -> List[Tuple[int, int, float]]:\n",
        "    n = X_csr.shape[0]\n",
        "    if n == 1:\n",
        "        return []\n",
        "    in_tree = cp.zeros(n, dtype=cp.bool_)\n",
        "    in_tree[0] = True\n",
        "\n",
        "    dist = sparse_pairwise_distances(X_csr[0:1], X_csr, metric=metric).ravel()\n",
        "    parent = cp.zeros(n, dtype=cp.int32)\n",
        "    edges = []\n",
        "    for _ in range(1, n):\n",
        "        u = int(cp.argmin(cp.where(in_tree, cp.inf, dist)).get())\n",
        "        edges.append((int(parent[u].get()), u, float(dist[u].get())))\n",
        "        in_tree[u] = True\n",
        "        du = sparse_pairwise_distances(X_csr[u:u+1], X_csr, metric=metric).ravel()\n",
        "        mask = (~in_tree) & (du < dist)\n",
        "        dist = cp.where(mask, du, dist)\n",
        "        parent = cp.where(mask, u, parent)\n",
        "    return edges\n",
        "\n",
        "###############################\n",
        "#   COARSENED GRAPH HELPERS   #\n",
        "###############################\n",
        "\n",
        "def min_distance(hypers: List[Set[int]], jdict: Dict[Tuple[int, int], float], A: List[int], rep_B: int) -> float:\n",
        "    \"\"\"Min distance from any hyperedge in A to representative rep_B.\"\"\"\n",
        "    return min(jdict.get((u, rep_B), jdict.get((rep_B, u), 1.0)) for u in A)\n",
        "\n",
        "###############################\n",
        "#              MAIN           #\n",
        "###############################\n",
        "\n",
        "def main():\n",
        "    t0 = time.perf_counter()\n",
        "    H, labels, hypers = build_incidence_and_labels()\n",
        "    load_time = time.perf_counter() - t0\n",
        "\n",
        "    # build partitions\n",
        "    part: Dict[int, List[int]] = defaultdict(list)\n",
        "    for idx, lab in enumerate(labels):\n",
        "        part[int(lab)].append(idx)\n",
        "\n",
        "    # compute Jaccard matrix for entire set using helper from original pipeline\n",
        "    from typing import Dict as _Dict, Tuple as _Tuple\n",
        "    def build_bipartite_df(hs: List[Set[int]]):\n",
        "        src, dst = [], []\n",
        "        HN = len(hs)\n",
        "        for hid, he in enumerate(hs):\n",
        "            for n in he:\n",
        "                src.append(hid)\n",
        "                dst.append(n + HN)\n",
        "        return cudf.DataFrame({\"src\": src, \"dst\": dst})\n",
        "\n",
        "    def get_global_jaccard(hs: List[Set[int]]) -> _Dict[_Tuple[int,int], float]:\n",
        "        HN = len(hs)\n",
        "        if HN <= 1:\n",
        "            return {}\n",
        "        df = build_bipartite_df(hs)\n",
        "        G = cugraph.Graph(directed=False)\n",
        "        G.from_cudf_edgelist(df, source='src', destination='dst', renumber=False)\n",
        "        jdf = cugraph.jaccard(G)\n",
        "        mask = (jdf['first'] < HN) & (jdf['second'] < HN)\n",
        "        jdf = jdf[mask]\n",
        "        jdf['weight'] = (1.0 - jdf['jaccard_coeff']).astype('float32')\n",
        "        pdf = jdf[['first','second','weight']].to_pandas()\n",
        "        return {(int(u), int(v)): float(w) for u, v, w in pdf.itertuples(index=False)}\n",
        "\n",
        "    jdict = get_global_jaccard(hypers)\n",
        "\n",
        "    # Component MSTs\n",
        "    H_gpu = cp_csr(H.astype(np.float32))\n",
        "    comp_edges: List[Tuple[int,int,float]] = []\n",
        "    comp_weight = 0.0\n",
        "    for lab, idxs in part.items():\n",
        "        sub_gpu = H_gpu[idxs]\n",
        "        e = prim_exact_mst(sub_gpu)\n",
        "        comp_edges.extend([(idxs[s], idxs[d], w) for s,d,w in e])\n",
        "        comp_weight += sum(w for _,_,w in e)\n",
        "\n",
        "    # Coarsened MST\n",
        "    reps = {lab: idxs[0] for lab, idxs in part.items()}\n",
        "    labs = list(part.keys())\n",
        "    rows = []; cols = []; wts = []\n",
        "    for i, li in enumerate(labs[:-1]):\n",
        "        Pi = part[li]\n",
        "        for j, lj in enumerate(labs[i+1:], i+1):\n",
        "            Pj = part[lj]\n",
        "            wi = min_distance(hypers, jdict, Pi, reps[lj])\n",
        "            wj = min_distance(hypers, jdict, Pj, reps[li])\n",
        "            w = min(wi, wj, 1.0)\n",
        "            rows.append(i); cols.append(j); wts.append(w)\n",
        "    from scipy.sparse.csgraph import minimum_spanning_tree\n",
        "    M = sp_csr((wts, (rows, cols)), shape=(len(labs), len(labs)))\n",
        "    M = M + M.T\n",
        "    mst_small = minimum_spanning_tree(M).toarray()\n",
        "    coarse_edges = []\n",
        "    coarse_weight = 0.0\n",
        "    for i in range(len(labs)):\n",
        "        for j in range(len(labs)):\n",
        "            w = mst_small[i, j]\n",
        "            if w > 0:\n",
        "                coarse_edges.append((reps[labs[i]], reps[labs[j]], float(w)))\n",
        "                coarse_weight += w\n",
        "\n",
        "    # Combined\n",
        "    total_mfc = comp_weight + coarse_weight\n",
        "\n",
        "    print(f\"Load time: {load_time:.2f}s  |  Hyperedges: {len(hypers):,}  Partitions: {len(part)}\")\n",
        "    print(f\"Component MST weight: {comp_weight:.4f}  |  Coarse MST weight: {coarse_weight:.4f}  |  MFC total: {total_mfc:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8UaUkYSEPcF",
        "outputId": "017994dd-e9cf-4b98-9afd-954e633d823d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load time: 3.48s  |  Hyperedges: 233,202  Partitions: 150\n",
            "Component MST weight: 144723.8125  |  Coarse MST weight: 132.5172  |  MFC total: 144856.3297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip '/content/outputs_trivago.zip' '/content/outputs_trivago'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFnpSqa3Ew9_",
        "outputId": "1efe316e-2911-4ed0-c731-b5bb7ce0bfb7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/outputs_trivago/ (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Set\n",
        "\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix as sp_csr\n",
        "from cupyx.scipy.sparse import csr_matrix as cp_csr\n",
        "from cuml.metrics import sparse_pairwise_distances\n",
        "import cugraph\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "###############################\n",
        "#          SETTINGS           #\n",
        "###############################\n",
        "DATA_DIR = Path(\"/content/trivago-clicks\")\n",
        "DATASET_PREFIX = \"trivago-clicks\"  # file name stem\n",
        "\n",
        "OUT_DIR = Path(\"outputs_trivago\")\n",
        "PARTITION_DIR = OUT_DIR / \"partitions\"\n",
        "PARTITION_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "###############################\n",
        "#     DATA LOADING HELPERS    #\n",
        "###############################\n",
        "\n",
        "def load_hyperedges(path: Path) -> List[Set[int]]:\n",
        "    \"\"\"Return list of hyperedges as sets of ints (node IDs).\"\"\"\n",
        "    hypers: List[Set[int]] = []\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                hypers.append(set(map(int, line.split(\",\"))))\n",
        "    return hypers\n",
        "\n",
        "\n",
        "def load_node_labels(path: Path) -> List[str]:\n",
        "    \"\"\"Return list where index=node ID and value=label string.\"\"\"\n",
        "    with open(path) as f:\n",
        "        return [ln.strip() for ln in f]\n",
        "\n",
        "\n",
        "def majority_label(hyperedge: Set[int], node_labels: List[str]) -> str:\n",
        "    \"\"\"Label hyperedge by the most frequent node label (lexicographic tie‑break).\"\"\"\n",
        "    cnt = Counter(node_labels[n] if n < len(node_labels) else \"UNKNOWN\" for n in hyperedge)\n",
        "    if not cnt:\n",
        "        return \"UNKNOWN\"\n",
        "    max_c = max(cnt.values())\n",
        "    return min(lbl for lbl, c in cnt.items() if c == max_c)\n",
        "\n",
        "###############################\n",
        "#   GRAPH + PARTITION BUILD   #\n",
        "###############################\n",
        "\n",
        "def build_incidence_and_labels() -> Tuple[sp_csr, np.ndarray, List[Set[int]]]:\n",
        "    \"\"\"Load dataset, fix 1‑based IDs, build CSR incidence & int labels array.\"\"\"\n",
        "    hyper_path = DATA_DIR / f\"hyperedges-{DATASET_PREFIX}.txt\"\n",
        "    label_path = DATA_DIR / f\"node-labels-{DATASET_PREFIX}.txt\"\n",
        "\n",
        "    hypers = load_hyperedges(hyper_path)\n",
        "    node_labels = load_node_labels(label_path)\n",
        "\n",
        "    # detect 1‑based: if ANY node id >= len(node_labels)\n",
        "    n_nodes_file = len(node_labels)\n",
        "    if any(n >= n_nodes_file for he in hypers for n in he):\n",
        "        hypers = [set(n - 1 for n in he) for he in hypers]\n",
        "\n",
        "    # incidence rows/cols\n",
        "    rows, cols = [], []\n",
        "    for hid, he in enumerate(hypers):\n",
        "        rows.extend([hid] * len(he))\n",
        "        cols.extend(he)\n",
        "    data = np.ones(len(rows), dtype=np.int8)\n",
        "    n_hyperedges = len(hypers)\n",
        "    n_nodes = max(cols) + 1\n",
        "    H = sp_csr((data, (rows, cols)), shape=(n_hyperedges, n_nodes))\n",
        "\n",
        "    # string labels → ints\n",
        "    str_labs = [majority_label(he, node_labels) for he in hypers]\n",
        "    uniq = {lbl: i for i, lbl in enumerate(sorted(set(str_labs)))}\n",
        "    int_labels = np.array([uniq[lbl] for lbl in str_labs], dtype=np.int32)\n",
        "    return H, int_labels, hypers\n",
        "\n",
        "###############################\n",
        "#   INTERNAL MST (GPU Prim)   #\n",
        "###############################\n",
        "\n",
        "def prim_exact_mst(X_csr: cp_csr, metric: str = \"jaccard\") -> List[Tuple[int, int, float]]:\n",
        "    n = X_csr.shape[0]\n",
        "    if n == 1:\n",
        "        return []\n",
        "    in_tree = cp.zeros(n, dtype=cp.bool_)\n",
        "    in_tree[0] = True\n",
        "\n",
        "    dist = sparse_pairwise_distances(X_csr[0:1], X_csr, metric=metric).ravel()\n",
        "    parent = cp.zeros(n, dtype=cp.int32)\n",
        "    edges = []\n",
        "    for _ in range(1, n):\n",
        "        u = int(cp.argmin(cp.where(in_tree, cp.inf, dist)).get())\n",
        "        edges.append((int(parent[u].get()), u, float(dist[u].get())))\n",
        "        in_tree[u] = True\n",
        "        du = sparse_pairwise_distances(X_csr[u:u+1], X_csr, metric=metric).ravel()\n",
        "        mask = (~in_tree) & (du < dist)\n",
        "        dist = cp.where(mask, du, dist)\n",
        "        parent = cp.where(mask, u, parent)\n",
        "    return edges\n",
        "\n",
        "###############################\n",
        "#   COARSENED GRAPH HELPERS   #\n",
        "###############################\n",
        "\n",
        "def min_distance(hypers: List[Set[int]], jdict: Dict[Tuple[int, int], float], A: List[int], rep_B: int) -> float:\n",
        "    \"\"\"Min distance from any hyperedge in A to representative rep_B.\"\"\"\n",
        "    return min(jdict.get((u, rep_B), jdict.get((rep_B, u), 1.0)) for u in A)\n",
        "\n",
        "###############################\n",
        "#              MAIN           #\n",
        "###############################\n",
        "\n",
        "def main():\n",
        "    timings: Dict[str, float] = {}\n",
        "\n",
        "    # STEP 0: load + preprocess -------------------------------------------------\n",
        "    t0 = time.perf_counter()\n",
        "    H, labels, hypers = build_incidence_and_labels()\n",
        "    timings[\"load_and_preprocess\"] = time.perf_counter() - t0\n",
        "\n",
        "    # STEP 1: build partitions --------------------------------------------------\n",
        "    t0 = time.perf_counter()\n",
        "    partitions: Dict[int, List[int]] = defaultdict(list)\n",
        "    for idx, lab in enumerate(labels):\n",
        "        partitions[int(lab)].append(idx)\n",
        "    timings[\"partition\"] = time.perf_counter() - t0\n",
        "\n",
        "    # persist partition mapping\n",
        "    OUT_DIR.mkdir(exist_ok=True)\n",
        "    mapping_path = OUT_DIR / \"hyperedge_partition_mapping.txt\"\n",
        "    with open(mapping_path, \"w\") as f:\n",
        "        for h_idx, lab in enumerate(labels):\n",
        "            f.write(f\"{h_idx}\t{lab}\")\n",
        "\n",
        "    # STEP 2: internal MSTs -----------------------------------------------------\n",
        "    H_gpu = cp_csr(H.astype(np.float32))\n",
        "    internal_edges: Dict[int, List[Tuple[int, int, float]]] = {}\n",
        "    internal_weights: Dict[int, float] = {}\n",
        "    mst_time = 0.0\n",
        "    for lab, nodes in partitions.items():\n",
        "        if len(nodes) == 1:\n",
        "            internal_edges[lab] = []\n",
        "            internal_weights[lab] = 0.0\n",
        "            continue\n",
        "        t0 = time.perf_counter()\n",
        "        sub_gpu = H_gpu[nodes]\n",
        "        edges = prim_exact_mst(sub_gpu)\n",
        "        mst_time += time.perf_counter() - t0\n",
        "        internal_edges[lab] = [(nodes[s], nodes[d], w) for s, d, w in edges]\n",
        "        internal_weights[lab] = sum(w for _, _, w in edges)\n",
        "        # write each partition MST\n",
        "        (PARTITION_DIR / f\"partition_{lab}_mst.txt\").write_text(\n",
        "            \"\".join(f\"{s} {d} {w}\" for s, d, w in internal_edges[lab]))\n",
        "    timings[\"component_msts\"] = mst_time\n",
        "\n",
        "    # STEP 3: coarsen graph + MST ----------------------------------------------\n",
        "    # build global jaccard dict (needed for coarsening distances)\n",
        "    from typing import Dict as _Dict, Tuple as _Tuple\n",
        "    def build_bipartite_df(hs: List[Set[int]]):\n",
        "        src, dst = [], []\n",
        "        HN = len(hs)\n",
        "        for hid, he in enumerate(hs):\n",
        "            for n in he:\n",
        "                src.append(hid)\n",
        "                dst.append(n + HN)\n",
        "        return cudf.DataFrame({\"src\": src, \"dst\": dst})\n",
        "\n",
        "    def get_global_jaccard(hs: List[Set[int]]) -> _Dict[_Tuple[int,int], float]:\n",
        "        HN = len(hs)\n",
        "        if HN <= 1:\n",
        "            return {}\n",
        "        df = build_bipartite_df(hs)\n",
        "        G = cugraph.Graph(directed=False)\n",
        "        G.from_cudf_edgelist(df, source='src', destination='dst', renumber=False)\n",
        "        jdf = cugraph.jaccard(G)\n",
        "        mask = (jdf['first'] < HN) & (jdf['second'] < HN)\n",
        "        jdf = jdf[mask]\n",
        "        jdf['weight'] = (1.0 - jdf['jaccard_coeff']).astype('float32')\n",
        "        pdf = jdf[['first','second','weight']].to_pandas()\n",
        "        return {(int(u), int(v)): float(w) for u, v, w in pdf.itertuples(index=False)}\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    jdict = get_global_jaccard(hypers)\n",
        "\n",
        "    # reps\n",
        "    reps = {lab: nodes[0] for lab, nodes in partitions.items()}\n",
        "    labs = list(partitions.keys())\n",
        "\n",
        "    rows = []; cols = []; wts = []\n",
        "    for i, li in enumerate(labs[:-1]):\n",
        "        Pi = partitions[li]\n",
        "        for j, lj in enumerate(labs[i+1:], i+1):\n",
        "            Pj = partitions[lj]\n",
        "            wi = min_distance(hypers, jdict, Pi, reps[lj])\n",
        "            wj = min_distance(hypers, jdict, Pj, reps[li])\n",
        "            w = min(wi, wj, 1.0)\n",
        "            rows.append(i); cols.append(j); wts.append(w)\n",
        "    from scipy.sparse.csgraph import minimum_spanning_tree\n",
        "    M = sp_csr((wts, (rows, cols)), shape=(len(labs), len(labs)))\n",
        "    M = M + M.T\n",
        "    timings[\"compute_coarsened_edges\"] = time.perf_counter() - t0\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "    mst_small = minimum_spanning_tree(M).toarray()\n",
        "    comp_mst_edges: List[Tuple[int,int,float]] = []\n",
        "    for i in range(len(labs)):\n",
        "        for j in range(len(labs)):\n",
        "            w = mst_small[i, j]\n",
        "            if w > 0:\n",
        "                comp_mst_edges.append((reps[labs[i]], reps[labs[j]], float(w)))\n",
        "    timings[\"mst_coarsened\"] = time.perf_counter() - t0\n",
        "\n",
        "    # STEP 4: combine ----------------------------------------------------------\n",
        "    combined_edges = [e for edges in internal_edges.values() for e in edges]\n",
        "    combined_edges.extend(comp_mst_edges)\n",
        "\n",
        "    approx_mfc_weight = sum(w for *_, w in combined_edges)\n",
        "    coarsened_weight = sum(internal_weights.values())\n",
        "\n",
        "    (OUT_DIR / \"combined_mst.txt\").write_text(\n",
        "        \"\".join(f\"{s} {d} {w}\" for s, d, w in combined_edges))\n",
        "\n",
        "    stats_lines = [\n",
        "        *[f\"{k}: {v:.4f} s\" for k, v in timings.items()],\n",
        "        f\"approx_MFC_weight: {approx_mfc_weight:.6f}\",\n",
        "        f\"coarsened_graph_weight: {coarsened_weight:.6f}\",\n",
        "    ]\n",
        "    (OUT_DIR / \"run_stats.txt\").write_text(\"\".join(stats_lines))\n",
        "\n",
        "    print(\"=== Timings (s) ===\")\n",
        "    for k, v in timings.items():\n",
        "        print(f\"{k:<25} {v:.4f}\")\n",
        "    print(\"approx MFC weight:\", approx_mfc_weight)\n",
        "    print(\"coarsened graph weight:\", coarsened_weight)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvmvcbTHPAnp",
        "outputId": "18e9bd24-e517-4d59-c6c2-b4954e553e33"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Timings (s) ===\n",
            "load_and_preprocess       3.9167\n",
            "partition                 0.0589\n",
            "component_msts            1091.1405\n",
            "compute_coarsened_edges   37.9934\n",
            "mst_coarsened             0.0044\n",
            "approx MFC weight: 144856.329700768\n",
            "coarsened graph weight: 144723.81247603893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip '/content/outputs_trivago.zip' '/content/outputs_trivago"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCOO93tIV3HN",
        "outputId": "6c3bbdf3-1c89-4f3d-e097-12aed76905cc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: unexpected EOF while looking for matching `''\n",
            "/bin/bash: -c: line 2: syntax error: unexpected end of file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# zip /content/outputs_trivago\n",
        "!tar -zcvf output_trivago.tar.gz> outputs_trivago\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNMAK7xnXpt9",
        "outputId": "61286ed2-bdbc-4ac8-b8d4-cec7e68cee75"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: outputs_trivago: Is a directory\n"
          ]
        }
      ]
    }
  ]
}